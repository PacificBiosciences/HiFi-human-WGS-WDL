[scheduler]
container_backend = slurm_singularity
# task_concurrency defaults to the number of processors on the system.
# since we submit the jobs to SLURM this is not necessary.
# higher numbers means miniwdl has to monitor more processes simultaneously
# which might impact performance.
task_concurrency=200
# This setting allows running tasks to continue, even if one other tasks fails.
# Useful in combination with call caching. Prevents wasting resources by
# cancelling jobs half-way that would probably succeed.
fail_fast = false

[file_io]
# This is required for the new ref_map and tertiary_map inputs.
allow_any_input = true

[call_cache]
# The following settings create a call cache under the current directory.
# This prevents wasting unnecessary resources on the cluster by rerunning
# jobs that have already succeeded.
put = true
get = true
dir = "$PWD/miniwdl_call_cache"

[task_runtime]
# Setting a 'maxRetries' default allows jobs that fail due to intermittent
# errors on the cluster to be retried.
## Requires miniwdl >= 1.9.0
command_shell = /bin/bash
defaults = {
        "maxRetries": 2,
        "docker": "ubuntu:20.04"
    }

[singularity]
# This plugin wraps the singularity backend. Make sure the settings are
# appropriate for your cluster.
exe = ["/usr/bin/singularity"]

# the miniwdl default options contain options to run as a fake root, which
# is not available on most clusters.
run_options = [
        "--containall",
        "--nv"
    ]

# Location of the singularity images (optional). The miniwdl-slurm plugin
# will set it to a directory inside $PWD. This location must be reachable
# for the submit nodes.
image_cache = "$PWD/miniwdl_singularity_cache"

[slurm]
# extra arguments passed to the srun command (optional).
extra_args="--partition compute --comment 'run with miniwdl'"